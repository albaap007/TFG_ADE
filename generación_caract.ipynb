{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAWMveGK40ac"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#import numpy as np\n",
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2n8vAH65Ixy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtGW4XQa5lWD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#train_data_dir = '/content/drive/My Drive/Curso_DL_CFP/proyecto/data/train'\n",
        "trainX = '/content/drive/My Drive/Curso_DL_CFP/proyecto/data_p/dataset_pandas/'\n",
        "testX = '/content/drive/My Drive/Curso_DL_CFP/proyecto/data_p/examples_pandas/'\n",
        "trainX_p = '/Users/albaprimoaparici/Desktop/dataset_pandas/'\n",
        "testX_p = '/Users/albaprimoaparici/Desktop//examples_pandas/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTDlhmR66gdi"
      },
      "outputs": [],
      "source": [
        "import keras.applications\n",
        "#from keras.applications import VGG16\n",
        "from keras.applications import imagenet_utils\n",
        "import keras.utils\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.engine\n",
        "#from keras.engine import Model\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "import keras.optimizers\n",
        "#from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "input_shape = (32, 32, 3) #(X)\n",
        "labelY = [\"indoor\", \"outdoor\"]\n",
        "lb = LabelBinarizer()\n",
        "labelY = lb.fit_transform(labelY)\n",
        "#labelY = to_categorical(labelY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OxtH5M1OSOi"
      },
      "outputs": [],
      "source": [
        "# Data generators para poder hacer data augmentation\n",
        "print('Usando real-time data augmentation.')\n",
        "datagen_train = ImageDataGenerator( #(X)\n",
        "    rotation_range=int(180*0.1),  #(X)\n",
        "    width_shift_range=0.1,  #(X)\n",
        "    height_shift_range=0.1,  #(X)\n",
        "    zoom_range=0.1,  #(X)\n",
        "    horizontal_flip=True, #(X)\n",
        "    validation_split=0.2,\n",
        "    rescale=1./255\n",
        "    #preprocessing_function = imagenet_utils.preprocess_input\n",
        ")\n",
        "\n",
        "datagen_test = ImageDataGenerator( #(X)\n",
        "    #preprocessing_function = imagenet_utils.preprocess_input\n",
        "    rescale=1./255\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md413YInQddI"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_datagen = datagen_train.flow_from_directory(directory = trainX,\n",
        "    target_size=(32, 32),\n",
        "    color_mode=\"rgb\",\n",
        "    classes=None,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    save_to_dir=None,\n",
        "    save_prefix=\"\",\n",
        "    save_format=\"png\",\n",
        "    follow_links=False,\n",
        "    subset=\"training\") #(X)\n",
        "\n",
        "val_datagen = datagen_train.flow_from_directory(directory = trainX,\n",
        "    target_size=(32, 32),\n",
        "    color_mode=\"rgb\",\n",
        "    classes=None,\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=None,\n",
        "    save_to_dir=None,\n",
        "    save_prefix=\"\",\n",
        "    save_format=\"png\",\n",
        "    follow_links=False,\n",
        "    subset=\"validation\")\n",
        "\n",
        "test_datagen = datagen_test.flow_from_directory(directory = testX,\n",
        "    target_size=(32, 32),\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=32,\n",
        "    shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhNsquP1Nalt"
      },
      "outputs": [],
      "source": [
        "#Importing library\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Input, Conv2D, Activation, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, MaxPool2D\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1000)\n",
        "\n",
        "#Instantiation\n",
        "AlexNet = Sequential()\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(2))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()\n",
        "\n",
        "# Compiling the model\n",
        "#optimizer=SGD(lr=0.01, decay=0.01/50, momentum=0.9, nesterov=True)\n",
        "print(\"[INFO]: Compilando red neuronal...\")\n",
        "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])\n",
        "\n",
        "#Entrenamos\n",
        "print(\"[INFO]: Entrenando red neuronal...\")\n",
        "H = AlexNet.fit(train_datagen, validation_data=val_datagen, epochs=100) \n",
        "\n",
        "# Montamos la unidad de Drive\n",
        "drive.mount('/content/drive') #(X)\n",
        "# Almacenamos el modelo empleando la función mdoel.save de Keras\n",
        "AlexNet.save(\"/content/drive/My Drive/Curso_DL_CFP/Modelos/deepCNN_fin.h5\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBctWerSv9S6"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import imutils\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "labelNames = [\"indoor\", \"outdoor\"]\n",
        "\n",
        "def predict_image(image, model):\n",
        "  # Creamos una copia sobre la que mostraremos el resultado (comando image.copy())\n",
        "  output = image.copy() #(X)\n",
        "  # Expandimos las dimensiones (32, 32, 3) a (1, 32, 32, 3)\n",
        "  image = np.expand_dims(image, axis=0) #(X)\n",
        " \n",
        "\n",
        "  # Clasificación de la imagen empleando el modelo\n",
        "  #print(\"[INFO]: Clasificando imagen...\")\n",
        "  # Realizamos la predicción\n",
        "  proba = AlexNet.predict(image) #(X)\n",
        "  #print(proba)\n",
        "  #print (np.max(proba))\n",
        "  # Nos quedamos con la clase que presente una probabilidad mayor y buscamos la etiqueta en el vector labelNames\n",
        "  idx = np.argmax(proba) \n",
        "  #print(idx)#(X)\n",
        "  label = labelNames[idx] \n",
        "  #print(label)#(X)\n",
        "  res = label\n",
        "  acc = np.max(proba)\n",
        "  \n",
        "  return res, acc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcy7kyfztw8H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Montamos la unidad de Drive\n",
        "drive.mount('/content/drive') #(X)\n",
        "\n",
        "# Selecciono imagen y la leo con OPENCV\n",
        "img_path = \"/content/drive/My Drive/Curso_DL_CFP/imagenes/prueba1.jpg\" # Path de Drive donde tengo la imagen (incluido el nombre de la misma) #(X)\n",
        "img_test = cv2.imread(img_path, cv2.IMREAD_COLOR) # Leo imagen con OPENCV\n",
        "img_test = cv2.cvtColor(img_test,cv2.COLOR_BGR2RGB) # Por defecto la carga en BGR, la convierto a RGB\n",
        "res = []\n",
        "\n",
        "\n",
        "# Pre-procesamos tal y como he hecho para la fase de entrenamiento con las muestras de CIFAR10\n",
        "img_test = img_test.astype(\"float\") / 255.0 #(X)\n",
        "# Re-escalamos la imagen al tamaño con el que fue entrenada la red (comando cv2.resize)\n",
        "img_test_res = cv2.resize(img_test, (32, 32)) #(X)\n",
        "# Predecimos la imagen pasando como parámetros a la función predict_image: la imagen, el modelo y string con el GT\n",
        "res = predict_image(img_test_res, AlexNet)#(X)\n",
        "\n",
        "print(res[0])\n",
        "print(res[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZFbp5Ue-GJc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "# Montamos la unidad de Drive\n",
        "drive.mount('/content/drive') #(X)\n",
        "\n",
        "\n",
        "\n",
        "ImageDataPath = \"/content/drive/My Drive/Curso_DL_CFP/proyecto/dataset\"\n",
        "label = []\n",
        "post = []\n",
        "acc = []\n",
        "people = ''\n",
        "for dir1 in os.listdir(ImageDataPath):\n",
        "    print(dir1)\n",
        "    if (dir1 == '.DS_Store' or dir1 == '.ipynb_checkpoints'): \n",
        "        print('borrrrraaaaaameeeeee')\n",
        "    \n",
        "        \n",
        "    else:\n",
        "        files = os.listdir(ImageDataPath + '/' + dir1)\n",
        "        fotos = []\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                original_image = cv2.imread(ImageDataPath + '/' + dir1 + '/'+ file, cv2.IMREAD_COLOR) # Leo imagen con OPENCV\n",
        "                if original_image is not None:\n",
        "                    # Convert image to grayscale\n",
        "                    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB) # Por defecto la carga en BGR, la convierto a RGB\n",
        "\n",
        "                    # Pre-procesamos tal y como he hecho para la fase de entrenamiento con las muestras de CIFAR10\n",
        "                    imgage = image.astype(\"float\") / 255.0 #(X)\n",
        "                    # Re-escalamos la imagen al tamaño con el que fue entrenada la red (comando cv2.resize)\n",
        "                    img_res = cv2.resize(imgage, (32, 32)) #(X)\n",
        "                    # Predecimos la imagen pasando como parámetros a la función predict_image: la imagen, el modelo y string con el GT\n",
        "                    res = predict_image(img_res, AlexNet)#(X)\n",
        "\n",
        "                    print(file)\n",
        "                    print(res)\n",
        "               \n",
        "                    label.append(res[0])\n",
        "                    post.append(file)\n",
        "                    acc.append(res[1])\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "                else:\n",
        "                    print(f'En error occurred while trying to load image')\n",
        "             \n",
        "\n",
        "\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcUeOx8kelJD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6DF8MXQd7un"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "feature_space = []\n",
        "with open(\"/content/drive/My Drive/Curso_DL_CFP/proyecto/datos_nc_modelos/in_out.csv\", 'w', encoding='UTF-8') as archivo:\n",
        "        writer = csv.DictWriter(archivo, fieldnames=['post', 'label', 'acc'], delimiter=',')\n",
        "        writer.writeheader()\n",
        "        for tupla in zip(post, label, acc):\n",
        "          fila = {}\n",
        "          fila['post'] =  tupla[0]\n",
        "          fila['label'] = tupla[1]\n",
        "          fila['acc'] = tupla[2]\n",
        "          feature_space.append(fila)\n",
        "        \n",
        "        writer.writerows(feature_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UzRGYJtWoPGG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "# Montamos la unidad de Drive\n",
        "drive.mount('/content/drive') #(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def draw_found_faces(detected, image, color: tuple):\n",
        "    for (x, y, width, height) in detected:\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (x, y),\n",
        "            (x + width, y + height),\n",
        "            color,\n",
        "            thickness=2\n",
        "        )\n",
        "\n",
        "path_to_image=\"/content/drive/My Drive/Curso_DL_CFP/imagenes/prueba.png\"\n",
        "original_image = cv2.imread(path_to_image)\n",
        "print ('1')\n",
        "if original_image is not None:\n",
        "    # Convert image to grayscale\n",
        "    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Create Cascade Classifiers\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "    profile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_profileface.xml\")\n",
        "    \n",
        "    # Detect faces using the classifiers\n",
        "    detected_faces = face_cascade.detectMultiScale(image=image, scaleFactor=1.3, minNeighbors=4)\n",
        "    row, columns = np.shape(detected_faces)\n",
        "    print('N º caras detectadas de frente :  ', row)\n",
        "    #print(columns)\n",
        "    print (detected_faces)\n",
        "\n",
        "    detected_profiles = profile_cascade.detectMultiScale(image=image, scaleFactor=1.3, minNeighbors=4)\n",
        "    row, columns = np.shape(detected_profiles)\n",
        "    print('N º caras detectadas de perfil :  ', row)\n",
        "    #print(columns)\n",
        "    print(detected_profiles)\n",
        "    # Filter out profiles\n",
        "    profiles_not_faces = [x for x in detected_profiles if x not in detected_faces]\n",
        "    print('4')\n",
        "    # Draw rectangles around faces on the original, colored image\n",
        "    draw_found_faces(detected_faces, original_image, (0, 255, 0)) # RGB - green\n",
        "    draw_found_faces(detected_profiles, original_image, (0, 0, 255)) # RGB - red\n",
        "    print('5')\n",
        "    # Open a window to display the results\n",
        "    #cv2.imshow(f'Detected Faces in {path_to_image}', original_image)\n",
        "    cv2_imshow(original_image)\n",
        "    # The window will close as soon as any key is pressed (not a mouse click)\n",
        "    cv2.waitKey(0) \n",
        "    cv2.destroyAllWindows()\n",
        "else:\n",
        "    print(f'En error occurred while trying to load {path_to_image}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNC08fLqRhmY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "# Montamos la unidad de Drive\n",
        "drive.mount('/content/drive') #(X)\n",
        "\n",
        "\n",
        "\n",
        "ImageDataPath = \"/content/drive/My Drive/Curso_DL_CFP/proyecto/dataset\"\n",
        "faces = []\n",
        "post = []\n",
        "is_people = []\n",
        "people = 'false'\n",
        "for dir1 in os.listdir(ImageDataPath):\n",
        "    print(dir1)\n",
        "    if (dir1 == '.DS_Store' or dir1 == '.ipynb_checkpoints'): \n",
        "        print('borrrrraaaaaameeeeee')\n",
        "    \n",
        "        \n",
        "    else:\n",
        "        files = os.listdir(ImageDataPath + '/' + dir1)\n",
        "        fotos = []\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\"):\n",
        "                original_image = cv2.imread(ImageDataPath + '/' + dir1 + '/'+ file)\n",
        "                if original_image is not None:\n",
        "                    # Convertir imagen a escala de grises \n",
        "                    image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                    # Creando clasificador\n",
        "                    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "    \n",
        "                    # Detección de rostros usando el clasificador\n",
        "                    detected_faces = face_cascade.detectMultiScale(image=image, scaleFactor=1.3, minNeighbors=4)\n",
        "                    \n",
        "                      try:\n",
        "                        row, columns = np.shape(detected_faces)\n",
        "                        print('N º caras detectadas de frente :  ', row)\n",
        "                        people = 'true'\n",
        "                      except (ValueError):\n",
        "                        print (len(np.shape(detected_faces)))\n",
        "                        row = 0\n",
        "                        people = 'indeterminante'\n",
        "                    \n",
        "                    faces.append(row)\n",
        "                    post.append(file)\n",
        "                    is_people.append(people)\n",
        "                    print (detected_faces)\n",
        "\n",
        "\n",
        "\n",
        "                else:\n",
        "                    print(f'En error occurred while trying to load {original_image}')\n",
        "             \n",
        "\n",
        "\n",
        "print(faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzVbHsIu1dww"
      },
      "outputs": [],
      "source": [
        "print(faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6ctBXzh2dnX"
      },
      "outputs": [],
      "source": [
        "print(post)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1W4Y7zJ5u7C"
      },
      "outputs": [],
      "source": [
        "feature_faces = []\n",
        "for tupla in zip(post, faces, is_people): #obtenemos la tupla en cada iteración\n",
        "  print(tupla[0], tupla[1], tupla[2])\n",
        "  feature_faces.append(tupla)\n",
        "print(feature_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TnrNVFy-oTG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2D6DD_u7JYr"
      },
      "outputs": [],
      "source": [
        "import csv \n",
        "feature_faces = []\n",
        "with open(\"/content/drive/My Drive/Curso_DL_CFP/proyecto/datos_nc_modelos/num_faces.csv\", 'w', encoding='UTF-8') as archivo:\n",
        "        writer = csv.DictWriter(archivo, fieldnames=['post', 'faces', 'is_people'], delimiter=',')\n",
        "        writer.writeheader()\n",
        "        for tupla in zip(post, faces, is_people):\n",
        "          fila = {}\n",
        "          fila['post'] =  tupla[0]\n",
        "          fila['faces'] = tupla[1]\n",
        "          fila['is_people'] = tupla[2]\n",
        "          feature_faces.append(fila)\n",
        "        \n",
        "        writer.writerows(feature_faces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fBBnYgCZ6sEF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "faceClassif = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "image = cv2.imread('/content/drive/My Drive/Curso_DL_CFP/imagenes/prueba.png')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "faces = faceClassif.detectMultiScale(gray,\n",
        "  scaleFactor=1.1,\n",
        "  minNeighbors=5)\n",
        "for (x,y,w,h) in faces:\n",
        "  cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "generación_caract.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}